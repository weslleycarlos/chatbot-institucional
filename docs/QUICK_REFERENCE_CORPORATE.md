# ğŸ“Š Quick Reference - ImplementaÃ§Ã£o Corporativa vs Cloud

## Comparativo de Arquiteturas

### Cloud Atual (APIs Pagas)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Vantagens                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ âœ… Zero setup/manutenÃ§Ã£o     â”‚
â”‚ âœ… Escalabilidade automÃ¡tica â”‚
â”‚ âœ… Modelos SOTA (GPT-4, etc) â”‚
â”‚ âœ… Suporte 24/7              â”‚
â”‚ âŒ Custo alto ($5-20/M req)  â”‚
â”‚ âŒ Dados na cloud            â”‚
â”‚ âŒ Limite de taxa             â”‚
â”‚ âŒ LatÃªncia de rede          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Custo/Ano:** $60K - $240K (com alto uso)

---

### On-Premise (Open Source)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Vantagens                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ âœ… Custo 90% menor           â”‚
â”‚ âœ… Dados 100% privados       â”‚
â”‚ âœ… Offline (sem internet)    â”‚
â”‚ âœ… CustomizÃ¡vel              â”‚
â”‚ âœ… Sem limites de taxa       â”‚
â”‚ âš ï¸ Requer setup/manutenÃ§Ã£o   â”‚
â”‚ âš ï¸ Modelos menos poderosos   â”‚
â”‚ âš ï¸ Precisa suporte tÃ©cnico   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Custo/Ano:** $10K - $36K (one-time + operacional)

---

## Decision Matrix

**Use Cloud APIs se:**
- Documentos nÃ£o-sensÃ­veis
- OrÃ§amento flexÃ­vel
- MÃ¡xima qualidade importante
- Poucos usuÃ¡rios (< 100/mÃªs)

**Use On-Premise se:**
- ğŸ” Dados confidenciais/jurÃ­dicos
- ğŸ’° OrÃ§amento limitado
- ğŸš€ Alta utilizaÃ§Ã£o (> 10K req/mÃªs)
- ğŸ¢ Ambiente corporativo fechado
- ğŸ”Œ Possibilidade de downtime

---

## Guia de Modelos (Escolha RÃ¡pida)

### Para POC (Prototipagem)

```
LLM: llama2 (3.8B)
Embeddings: all-MiniLM-L6-v2 (80MB)
DB: SQLite (local)
Recursos: 2 CPU, 4GB RAM
Custo: $0
```

### Para ProduÃ§Ã£o Leve

```
LLM: Mistral (7B) ou Neural-Chat (7B)
Embeddings: bge-base-pt-v1.5 (438MB)
DB: PostgreSQL + pgvector
Recursos: 4 CPU, 16GB RAM
Custo: ~$100/mÃªs on-prem
```

### Para ProduÃ§Ã£o Robusto

```
LLM: OpenHermes (34B) ou Dolphin-Mixtral (8.7B)
Embeddings: bge-large-pt-v1.5 (1.2GB)
DB: PostgreSQL + pgvector + Milvus
Recursos: 8-16 CPU, 64GB RAM + GPU (NVIDIA)
Custo: ~$500/mÃªs on-prem
```

---

## Modelos Recomendados por Caso

### 1. Instituto PÃºblico / Tribunal

**Requisitos:**
- Documentos jurÃ­dicos
- Conformidade legal
- HistÃ³rico de requisiÃ§Ãµes
- Offline capability

**RecomendaÃ§Ã£o:**
```
LLM: openhermes-neural-chat-pt (7B portuguÃªs)
Embeddings: legal-bert-base (para jurÃ­dico)
BD: PostgreSQL + pgvector
Auditoria: Full compliance logging
Multi-tenant: Sim, com RBAC
Custo: $300-500/mÃªs
```

### 2. Universidade / Biblioteca Digital

**Requisitos:**
- Muitos documentos
- MÃºltiplos idiomas
- Busca semÃ¢ntica forte
- AnÃ¡lise de similaridade

**RecomendaÃ§Ã£o:**
```
LLM: mistral-instruct (7B) multilÃ­ngue
Embeddings: multilingual-e5-base (768D)
BD: Milvus (escalÃ¡vel para 1M+ docs)
Analytics: Sim, com relatÃ³rios
Custo: $200-400/mÃªs
```

### 3. Empresa Privada / Knowledge Base

**Requisitos:**
- Propriedade intelectual
- IntegraÃ§Ã£o com Sharepoint
- AnÃ¡lise de sentimento
- Fine-tuning possÃ­vel

**RecomendaÃ§Ã£o:**
```
LLM: mistral (7B) + LoRA finetuned
Embeddings: bge-large-pt-v1.5 (alta qualidade)
BD: PostgreSQL + pgvector + Milvus
Analytics: Completo
Fine-tuning: Sim, com LoRA
Custo: $400-800/mÃªs
```

### 4. Startup / MVP

**Requisitos:**
- Rapidez
- Baixo custo
- IteraÃ§Ã£o rÃ¡pida
- EscalÃ¡vel depois

**RecomendaÃ§Ã£o:**
```
LLM: Ollama + mistral (7B)
Embeddings: sentence-transformers (local)
BD: SQLite â†’ PostgreSQL (depois)
Analytics: BÃ¡sico
Custo: $50-100/mÃªs (cloud mÃ­nimo)
```

---

## Checklist de ImplementaÃ§Ã£o

### Semana 1: Infraestrutura

- [ ] Provisionar servidor (bare metal ou cloud)
- [ ] Instalar Ubuntu 22.04 LTS
- [ ] Configurar SSH/VPN
- [ ] Instalar Docker + Docker Compose
- [ ] Instalar NVIDIA drivers (se houver GPU)
- [ ] Configurar storage

### Semana 2: ServiÃ§os Base

- [ ] Deploy PostgreSQL
- [ ] Instalar pgvector extension
- [ ] Deploy Ollama
- [ ] Baixar modelo LLM (mistral/openhermes)
- [ ] Deploy Redis
- [ ] Testes de conectividade

### Semana 3: AplicaÃ§Ã£o

- [ ] Atualizar backend para usar Ollama
- [ ] Atualizar embeddings (bge-pt-v1.5)
- [ ] Migrar Chroma â†’ PostgreSQL + pgvector
- [ ] Testes de RAG pipeline
- [ ] Configurar logging/monitoring

### Semana 4: ProduÃ§Ã£o

- [ ] Configurar Nginx + HTTPS
- [ ] Backup automÃ¡tico (PostgreSQL)
- [ ] Monitoring (Prometheus + Grafana)
- [ ] DocumentaÃ§Ã£o deployment
- [ ] Treinamento usuÃ¡rios

---

## Performance Benchmarks

### Tempo de Resposta (ms)

| Etapa | CPU/GPU |
|-------|---------|
| Query variations | 30-50ms |
| Semantic search (k=20) | 100-300ms |
| BM25 search | 50-100ms |
| LLM inference (mistral 7B) | 2000-5000ms |
| Total por query | 2.5-5.5s |

**Com GPU (RTX 4090):**
- LLM inference: 500-1500ms
- Total: 1-2s por query

### Throughput

| ConfiguraÃ§Ã£o | Queries/seg | LatÃªncia p95 |
|--------------|------------|-------------|
| CPU 4c, 16GB | ~0.2-0.5 q/s | 3-5s |
| CPU 8c, 32GB | ~0.5-1 q/s | 2-3s |
| + GPU RTX3060 | ~1-2 q/s | 1-2s |
| + GPU RTX4090 | ~3-5 q/s | 0.5-1s |

---

## Troubleshooting Comum

### Ollama muito lento

```bash
# Verificar se estÃ¡ usando GPU
ollama list
nvidia-smi

# Se nÃ£o, ativar GPU
export OLLAMA_NUM_GPU=32  # Max GPUs disponÃ­veis
ollama serve
```

### PostgreSQL + pgvector lento

```sql
-- Criar Ã­ndice adequado
CREATE INDEX idx_chunks_embedding 
ON chunks USING ivfflat (embedding vector_cosine_ops);

-- Analisar query
EXPLAIN ANALYZE
SELECT * FROM chunks
ORDER BY embedding <-> '[0.1, 0.2, ...]'::vector
LIMIT 10;

-- Se ainda lento, aumentar probes
SET ivfflat.probes = 40;
```

### OOM (Out of Memory)

```bash
# Verificar uso
free -h
top -p $(pidof -x python)

# SoluÃ§Ã£o:
# 1. Usar modelo menor (llama2 vs openhermes)
# 2. QuantizaÃ§Ã£o (GGUF Q4)
# 3. Aumentar RAM
# 4. Usar swap (temporÃ¡rio)

# Criar swap de 32GB
sudo fallocate -l 32G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile
```

---

## Recursos Ãšteis

### DocumentaÃ§Ã£o

- **Ollama**: https://github.com/ollama/ollama
- **LangChain**: https://python.langchain.com/
- **pgvector**: https://github.com/pgvector/pgvector
- **Sentence Transformers**: https://www.sbert.net/

### Modelos DisponÃ­veis

```bash
# Listar modelos
ollama list

# Pull de modelos
ollama pull mistral          # 3.8GB
ollama pull openhermes       # 13GB
ollama pull dolphin-mixtral  # 8.7GB
ollama pull neural-chat      # 4.7GB
```

### Comunidades

- HuggingFace: https://huggingface.co/models
- Ollama Discord: https://discord.gg/ollama
- LangChain Community: https://discord.gg/langchain

---

## Calculadora de Recursos

```
RAM Total NecessÃ¡ria:
= 2GB (SO) 
+ Tamanho do Modelo LLM
+ 2GB (Ollama overhead)
+ 2-4GB (PostgreSQL)
+ 1GB (Redis)
+ 2GB (Buffer)

Exemplo Mistral 7B:
= 2 + 15 + 2 + 4 + 1 + 2 = 26GB RAM
Recomendado: 32GB

Exemplo OpenHermes 34B:
= 2 + 35 + 2 + 4 + 1 + 2 = 46GB RAM
Recomendado: 64GB
```

---

## Roadmap de Features

### MVP (MÃªs 1-2)
- âœ… Chat bÃ¡sico
- âœ… HistÃ³rico de conversas
- âœ… Admin panel

### v1.0 (MÃªs 3-4)
- ğŸ“‹ Multi-tenant
- ğŸ“Š Analytics dashboard
- ğŸ” Audit logs
- ğŸ“ Fine-tuning

### v2.0 (MÃªs 5+)
- ğŸ¤– Agents com ferramentas
- ğŸ”„ Retrieval feedback loop
- ğŸ“š Knowledge base management
- ğŸŒ API pÃºblica
- ğŸ”Œ IntegraÃ§Ãµes (SharePoint, LDAP)

---

**VersÃ£o:** 1.0  
**Data:** Janeiro 2026  
**Status:** Production Ready
