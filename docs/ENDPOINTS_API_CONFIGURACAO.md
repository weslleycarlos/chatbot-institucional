# Configura√ß√£o de Endpoints das APIs (Gemini, GROQ, OpenAI)

## üîß Onde est√£o as configura√ß√µes?

### **Arquivo: `backend/main.py` (linhas 233-252)**

```python
def _get_llm_instance():
    """
    Instancia o LLM conforme configurado em LLM_PROVIDER e LLM_MODEL
    Suporta: Gemini (Google), GROQ, OpenAI
    """
    if LLM_PROVIDER == "gemini":
        return ChatGoogleGenerativeAI(model=LLM_MODEL, temperature=0.2)
    
    elif LLM_PROVIDER == "groq":
        from langchain_groq import ChatGroq
        return ChatGroq(api_key=GROQ_API_KEY, model=LLM_MODEL, temperature=0.2)
    
    elif LLM_PROVIDER == "openai":
        from langchain_openai import ChatOpenAI
        return ChatOpenAI(api_key=OPENAI_API_KEY, model=LLM_MODEL, temperature=0.2)
```

## üìç Endpoints Utilizados (Autom√°ticos via LangChain)

As bibliotecas LangChain usam endpoints padr√£o e **n√£o requerem configura√ß√£o manual de URL**. Eles j√° est√£o pr√©-configurados:

### **1. Gemini (Google)**
- **Biblioteca**: `langchain-google-genai`
- **Endpoint**: `https://generativelanguage.googleapis.com/` (autom√°tico)
- **Autentica√ß√£o**: Via `GOOGLE_API_KEY`
- **Configura√ß√£o**: `ChatGoogleGenerativeAI(model=LLM_MODEL, temperature=0.2)`

**Seu setup atual**:
```env
GOOGLE_API_KEY=AIzaSyB0-Gu8pOLIxSWeG-AQbzAxcraapXr_YAc
LLM_PROVIDER=gemini
LLM_MODEL=gemini-2.5-flash
```

### **2. GROQ**
- **Biblioteca**: `langchain-groq`
- **Endpoint**: `https://api.groq.com/` (autom√°tico)
- **Autentica√ß√£o**: Via `GROQ_API_KEY`
- **Configura√ß√£o**: `ChatGroq(api_key=GROQ_API_KEY, model=LLM_MODEL, temperature=0.2)`

**Para usar GROQ**:
```env
GROQ_API_KEY=gsk_seu_token_aqui
LLM_PROVIDER=groq
LLM_MODEL=mixtral-8x7b-32768
```

### **3. OpenAI**
- **Biblioteca**: `langchain-openai`
- **Endpoint**: `https://api.openai.com/v1/` (autom√°tico)
- **Autentica√ß√£o**: Via `OPENAI_API_KEY`
- **Configura√ß√£o**: `ChatOpenAI(api_key=OPENAI_API_KEY, model=LLM_MODEL, temperature=0.2)`

**Para usar OpenAI**:
```env
OPENAI_API_KEY=sk_seu_token_aqui
LLM_PROVIDER=openai
LLM_MODEL=gpt-4o-mini
```

## ‚úÖ Como Verificar se o Caminho est√° Correto?

### **1. Teste de Importa√ß√£o B√°sico**
```bash
cd backend
.\venv\Scripts\activate.ps1
python -c "import main; print('‚úÖ Imports OK')"
```

Se aparecer erro de `ModuleNotFoundError`, falta instalar:
```bash
pip install langchain-groq langchain-openai langchain-google-genai
```

### **2. Teste de Conex√£o com a API**
```bash
python -c "
from main import _get_llm_instance
try:
    llm = _get_llm_instance()
    response = llm.invoke('Ol√°, tudo bem?')
    print('‚úÖ API funcionando!')
    print(f'Resposta: {response.content[:100]}...')
except Exception as e:
    print(f'‚ùå Erro: {e}')
"
```

### **3. Verificar Chave de API**
```bash
# Gemini
echo %GOOGLE_API_KEY%

# GROQ
echo %GROQ_API_KEY%

# OpenAI
echo %OPENAI_API_KEY%
```

### **4. Teste Completo do Sistema**
```bash
python -m uvicorn main:app --reload
# Acesse http://localhost:8000/docs
# Teste POST /chat com pergunta simples
```

## üîç O que Verificar no Console

### ‚úÖ **Sucesso**
```
ü§ñ LLM: GEMINI | Modelo: gemini-2.5-flash
‚úÖ Carregando modelo local: C:\...\modelo_local\all-MiniLM-L6-v2
‚úÖ Modelo LOCAL 'all-MiniLM-L6-v2' carregado com sucesso!
üîÑ Gerando varia√ß√µes da pergunta...
HyDE (doc hipot√©tico): ...
‚úÖ Total de documentos √∫nicos encontrados: 20
```

### ‚ùå **Erro - Chave n√£o configurada**
```
RuntimeError: GOOGLE_API_KEY n√£o configurada no .env
```
‚Üí Solu√ß√£o: Adicione `GOOGLE_API_KEY=...` no `.env`

### ‚ùå **Erro - Rede bloqueada**
```
ConnectionError: Max retries exceeded with url: /api.groq.com
```
‚Üí Motivo: Proxy da sua rede bloqueando GROQ
‚Üí Solu√ß√£o: Use Gemini (que voc√™ j√° tem configurado)

### ‚ùå **Erro - Modelo n√£o existe**
```
ValueError: Could not find model gpt-5000 (typo)
```
‚Üí Solu√ß√£o: Verifique `LLM_MODEL` no `.env` (modelos v√°lidos listados acima)

## üéØ Resumo das Localiza√ß√µes

| Componente | Localiza√ß√£o | Configura√ß√£o |
|-----------|-----------|--------------|
| **Provider selection** | `backend/main.py` linha 233 | `LLM_PROVIDER` env var |
| **Gemini endpoint** | `langchain-google-genai` (autom√°tico) | `GOOGLE_API_KEY` |
| **GROQ endpoint** | `langchain-groq` (autom√°tico) | `GROQ_API_KEY` |
| **OpenAI endpoint** | `langchain-openai` (autom√°tico) | `OPENAI_API_KEY` |
| **Model name** | `backend/main.py` linha 237, etc | `LLM_MODEL` env var |
| **Temperature** | `backend/main.py` linha 237, etc | `temperature=0.2` (hardcoded) |

## üìù Vari√°veis de Ambiente Requeridas

**Em `backend/.env`:**
```env
# Obrigat√≥rio: escolha o provider
LLM_PROVIDER=gemini

# Obrigat√≥rio: nome do modelo
LLM_MODEL=gemini-2.5-flash

# Chaves de API (apenas para o provider escolhido)
GOOGLE_API_KEY=...          # Para Gemini
GROQ_API_KEY=               # Para GROQ (deixar vazio se usar Gemini)
OPENAI_API_KEY=             # Para OpenAI (deixar vazio se usar Gemini)
```

## üöÄ Status Atual do Seu Sistema

```
‚úÖ GEMINI: Configurado e funcionando
   - Endpoint: api.generativelanguage.googleapis.com
   - API Key: ‚úì Presente
   - Modelo: gemini-2.5-flash
   
‚ùå GROQ: N√£o recomendado (sua rede bloqueada)
   - Endpoint: api.groq.com
   - API Key: Dispon√≠vel mas rede bloqueada por proxy
   
‚ö™ OpenAI: N√£o configurado
   - Endpoint: api.openai.com
   - API Key: N√£o definida
```

---

**Tl;dr**: Os endpoints est√£o autom√°ticos nas bibliotecas LangChain. Voc√™ s√≥ precisa garantir que:
1. ‚úÖ `LLM_PROVIDER` est√° definido
2. ‚úÖ `LLM_MODEL` √© v√°lido para esse provider
3. ‚úÖ Chave de API est√° no `.env`
4. ‚úÖ Rede permite conex√£o com a API (seu Gemini j√° funciona!)
