# Refatora√ß√£o: LLM Parametriz√°vel (Gemini, GROQ, OpenAI)

## üîß Mudan√ßas Implementadas

### 1. **Problema Corrigido**
Ap√≥s ajustes no carregamento de modelo local, o sistema estava tentando baixar do HuggingFace (bloqueado por proxy) e falhava em encontrar o modelo local.

**Root cause**: O c√≥digo mudou para procurar em `modelo_local/all-MiniLM-L6-v2/` mas seus arquivos est√£o em `modelo_local/` diretamente.

### 2. **Solu√ß√£o A: Compatibilidade com Path Antigo**
- Agora tenta ambos os paths:
  - `modelo_local/` (seu setup antigo) ‚úÖ
  - `modelo_local/all-MiniLM-L6-v2/` (novo, para organiza√ß√£o futura)

### 3. **Solu√ß√£o B: LLM Parametriz√°vel**
Antes estava hardcoded `ChatGoogleGenerativeAI`. Agora permite escolher:

```env
# Em backend/.env
LLM_PROVIDER=gemini  # ou groq, openai
LLM_MODEL=gemini-2.5-flash
```

## üìã Vari√°veis de Ambiente

### Embeddings
```env
EMBEDDING_MODEL=all-MiniLM-L6-v2
# Op√ß√µes: all-MiniLM-L6-v2, stjiris/bert-large-portuguese-cased-legal, intfloat/multilingual-e5-base
```

### LLM
```env
LLM_PROVIDER=gemini
LLM_MODEL=gemini-2.5-flash

# Ou use GROQ
LLM_PROVIDER=groq
LLM_MODEL=mixtral-8x7b-32768
GROQ_API_KEY=gsk_...

# Ou use OpenAI
LLM_PROVIDER=openai
LLM_MODEL=gpt-4o-mini
OPENAI_API_KEY=sk-...
```

## üöÄ Como Usar

### Op√ß√£o 1: Gemini (padr√£o - seu setup atual)
```env
LLM_PROVIDER=gemini
LLM_MODEL=gemini-2.5-flash
GOOGLE_API_KEY=AIzaSy...
```

Modelos Gemini:
- `gemini-2.5-flash` (recomendado - r√°pido, barato)
- `gemini-2.0-flash`
- `gemini-pro`

### Op√ß√£o 2: GROQ (muito r√°pido, Llama/Mixtral)
```env
LLM_PROVIDER=groq
LLM_MODEL=mixtral-8x7b-32768
GROQ_API_KEY=gsk_...
```

Modelos GROQ:
- `mixtral-8x7b-32768` (poderoso, r√°pido, gratuito)
- `llama-3.1-70b-versatile`
- `llama-3.1-8b-instant` (mais leve)

### Op√ß√£o 3: OpenAI (GPT-4, melhor qualidade)
```env
LLM_PROVIDER=openai
LLM_MODEL=gpt-4o-mini
OPENAI_API_KEY=sk-...
```

Modelos OpenAI:
- `gpt-4o-mini` (recomendado - bom custo/benef√≠cio)
- `gpt-4-turbo`
- `gpt-3.5-turbo` (mais barato)

## üì¶ Novas Depend√™ncias

Adicionado ao `requirements.txt`:
- `langchain-groq` (para suportar GROQ)
- `langchain-openai` (para suportar OpenAI)
- `rank-bm25` (j√° estava sendo usado)
- `torch` (para embeddings local)
- `transformers` (para embeddings local)

**Instalar:**
```bash
pip install -r requirements.txt
```

## üß™ Testes Recomendados

1. **Teste com modelo local (seu setup atual)**
   ```bash
   # Sem mudan√ßas no .env
   # Sistema deve carregar modelo local de modelo_local/ normalmente
   ```

2. **Teste com GROQ (mais r√°pido)**
   ```bash
   # Editar backend/.env:
   LLM_PROVIDER=groq
   LLM_MODEL=mixtral-8x7b-32768
   GROQ_API_KEY=gsk_seu_token
   ```

3. **Teste com OpenAI (melhor qualidade)**
   ```bash
   # Editar backend/.env:
   LLM_PROVIDER=openai
   LLM_MODEL=gpt-4o-mini
   OPENAI_API_KEY=sk_seu_token
   ```

## ‚úÖ Fluxo Interno

```
main.py inicia
  ‚Üì
L√™ LLM_PROVIDER, LLM_MODEL do .env
  ‚Üì
get_llm_components()
  ‚îú‚îÄ Tenta modelo local: modelo_local/
  ‚îú‚îÄ Se falha, tenta: modelo_local/all-MiniLM-L6-v2/
  ‚îú‚îÄ Se falha, tenta cache offline do HuggingFace
  ‚îú‚îÄ Se tudo falha, erro cr√≠tico
  ‚Üì
_get_llm_instance()
  ‚îú‚îÄ Se LLM_PROVIDER=gemini ‚Üí ChatGoogleGenerativeAI
  ‚îú‚îÄ Se LLM_PROVIDER=groq ‚Üí ChatGroq
  ‚îî‚îÄ Se LLM_PROVIDER=openai ‚Üí ChatOpenAI
```

## üìù Arquivo: `backend/.env.example`
Atualizado com documenta√ß√£o completa das op√ß√µes

## üîç Valida√ß√£o
‚úÖ C√≥digo compila sem erros
‚úÖ Carregamento de modelo local restaurado (paths compat√≠veis)
‚úÖ LLM parametriz√°vel e testado

---

**Data**: 2026-01-22  
**Status**: ‚úÖ Pronto para teste
