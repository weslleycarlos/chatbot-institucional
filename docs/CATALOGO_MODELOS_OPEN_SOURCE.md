# ğŸ“š CatÃ¡logo de Modelos Open Source

ReferÃªncia tÃ©cnica de modelos LLM e embeddings recomendados para produÃ§Ã£o.

---

## ğŸ§  Modelos LLM

### Tier 1: RÃ¡pido e Leve (POC)

#### Llama 2 - 7B Chat
- **Tamanho:** 3.8GB (quantizado Q4)
- **Velocidade:** âš¡âš¡âš¡ (muito rÃ¡pido)
- **Qualidade:** â­â­ (bÃ¡sica)
- **PortuguÃªs:** Regular
- **Download:**
  ```bash
  ollama pull llama2
  # ou
  ollama pull llama2-uncensored
  ```
- **Recomendado para:** Prototipagem, teste rÃ¡pido
- **Pros:** Muito rÃ¡pido, confiÃ¡vel, open source
- **Cons:** Qualidade baixa, portuguÃªs ruim

---

### Tier 2: Balanceado (ProduÃ§Ã£o PadrÃ£o)

#### Mistral 7B Instruct
- **Tamanho:** 3.8GB (Q4)
- **Velocidade:** âš¡âš¡ (rÃ¡pido)
- **Qualidade:** â­â­â­ (boa)
- **PortuguÃªs:** Bom
- **Download:**
  ```bash
  ollama pull mistral
  ```
- **Recomendado para:** Maioria dos casos corporativos
- **Pros:** Excelente trade-off velocidade/qualidade, multilÃ­ngue
- **Cons:** NÃ£o tÃ£o poderoso quanto modelos 34B+

#### Neural Chat 7B
- **Tamanho:** 4.7GB
- **Velocidade:** âš¡âš¡ (rÃ¡pido)
- **Qualidade:** â­â­â­ (boa)
- **PortuguÃªs:** Excelente (treinado com dados PT-BR)
- **Download:**
  ```bash
  ollama pull neural-chat
  ```
- **Recomendado para:** Chatbots em portuguÃªs
- **Pros:** Ã“timo para conversaÃ§Ã£o, portuguÃªs nativo
- **Cons:** Menor em contexto (2048 tokens)

#### Dolphin 2.6 Mixtral
- **Tamanho:** 8.7GB
- **Velocidade:** âš¡ (mÃ©dio)
- **Qualidade:** â­â­â­â­ (excelente)
- **PortuguÃªs:** Excelente
- **Download:**
  ```bash
  ollama pull dolphin-mixtral
  ```
- **Recomendado para:** AnÃ¡lise de documentos, RAG
- **Pros:** Excelente compreensÃ£o, bom portuguÃªs
- **Cons:** Mais lento que 7B

---

### Tier 3: Robusto (ProduÃ§Ã£o CrÃ­tica)

#### OpenHermes 2.5 34B
- **Tamanho:** 19GB (Q4)
- **Velocidade:** âš¡ (lento)
- **Qualidade:** â­â­â­â­ (excelente)
- **PortuguÃªs:** Excelente
- **Download:**
  ```bash
  ollama pull openhermes
  ```
- **Recomendado para:** Documentos complexos, sistemas crÃ­ticos
- **Pros:** Melhor qualidade, excelente portuguÃªs, suporta instruÃ§Ãµes complexas
- **Cons:** Requer 64GB RAM + GPU

#### Llama 2 70B Chat
- **Tamanho:** 39GB (Q4)
- **Velocidade:** âš¡ (muito lento)
- **Qualidade:** â­â­â­â­â­ (excelente)
- **PortuguÃªs:** Excelente
- **Download:**
  ```bash
  ollama pull llama2-70b
  ```
- **Recomendado para:** MÃ¡xima qualidade requerida
- **Pros:** Melhor em classe open source
- **Cons:** Requer 128GB+ RAM ou GPU com 48GB+

---

### Tier 4: Especializado

#### Nous Hermes 2 Mixtral 8x7B
- **Tamanho:** 48GB (full precision)
- **Velocidade:** âš¡ (mÃ©dio com GPU)
- **Qualidade:** â­â­â­â­â­
- **PortuguÃªs:** Excelente
- **SpecializaÃ§Ã£o:** AnÃ¡lise jurÃ­dica, RAG avanÃ§ado
- **Download:**
  ```bash
  ollama pull nous-hermes2-mixtral
  ```

#### Guanaco 65B
- **Tamanho:** 39GB (Q4)
- **Velocidade:** âš¡
- **Qualidade:** â­â­â­â­
- **PortuguÃªs:** Bom
- **SpecializaÃ§Ã£o:** MultilÃ­ngue, 200+ idiomas
- **Download:**
  ```bash
  ollama pull guanaco
  ```

---

## ğŸ“Š Modelos de Embeddings

### Comparativo Detalhado

| Modelo | DimensÃµes | Tamanho | Velocidade | PortuguÃªs | JurÃ­dico | RecomendaÃ§Ã£o |
|--------|-----------|---------|-----------|-----------|----------|--------------|
| all-MiniLM-L6-v2 | 384 | 80MB | âš¡âš¡âš¡ | Regular | â­ | MVP/POC |
| multilingual-e5-base | 768 | 438MB | âš¡âš¡ | Bom | â­â­ | PadrÃ£o |
| **bge-base-pt-v1.5** | 768 | 438MB | âš¡âš¡ | â­â­â­â­ | â­â­â­ | **Recomendado** |
| bge-large-pt-v1.5 | 1024 | 1.2GB | âš¡ | â­â­â­â­ | â­â­â­ | Alta qualidade |
| legal-bert-base | 768 | 440MB | âš¡âš¡ | Bom | â­â­â­â­ | Documentos jurÃ­dicos |
| jina-embeddings-v2 | 768 | 500MB | âš¡âš¡ | Bom | â­â­â­ | Docs longos (8K tokens) |
| multilingual-e5-large | 1024 | 2.2GB | âš¡ | Bom | â­â­ | MÃ¡xima qualidade |

### RecomendaÃ§Ãµes por Setor

#### ğŸ›ï¸ Setor PÃºblico / JurÃ­dico
```bash
# Embedding
EMBEDDING_MODEL=nlpaueb/legal-bert-base-uncased
# ou
EMBEDDING_MODEL=BAAI/bge-base-pt-v1.5

# LLM
OLLAMA_MODEL=openhermes  # 34B para mÃ¡xima qualidade
# ou
OLLAMA_MODEL=dolphin-mixtral  # 8.7B balanceado
```

#### ğŸ¢ Corporativo / Knowledge Base
```bash
# Embedding
EMBEDDING_MODEL=BAAI/bge-base-pt-v1.5

# LLM
OLLAMA_MODEL=mistral  # 7B rÃ¡pido
# ou
OLLAMA_MODEL=neural-chat  # 7B especial portuguÃªs
```

#### ğŸ“ EducaÃ§Ã£o / Pesquisa
```bash
# Embedding
EMBEDDING_MODEL=intfloat/multilingual-e5-base

# LLM
OLLAMA_MODEL=dolphin-mixtral  # Excelente compreensÃ£o
```

#### âš¡ Startup / MVP
```bash
# Embedding
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# LLM
OLLAMA_MODEL=mistral  # Melhor custo-benefÃ­cio
```

---

## ğŸ”§ InstalaÃ§Ã£o de Modelos

### Ollama (Recomendado)

```bash
# Instalar Ollama
curl https://ollama.ai/install.sh | sh

# Pull modelo
ollama pull mistral
ollama pull openhermes

# Listar
ollama list

# Usar em API
curl http://localhost:11434/api/generate -d '{
  "model": "mistral",
  "prompt": "OlÃ¡"
}'
```

### HuggingFace (Download Manual)

```bash
from transformers import AutoTokenizer, AutoModelForCausalLM

# Download modelo
model_name = "mistralai/Mistral-7B-Instruct-v0.1"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)

# Salvar localmente
model.save_pretrained("./modelos_local/mistral-7b")
tokenizer.save_pretrained("./modelos_local/mistral-7b")
```

### vLLM (Para Batch Inference)

```bash
# Instalar
pip install vllm

# Servidor
python -m vllm.entrypoints.openai.api_server \
  --model mistralai/Mistral-7B-Instruct-v0.1 \
  --tensor-parallel-size 2 \
  --gpu-memory-utilization 0.9

# Usar como OpenAI API
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "mistralai/Mistral-7B-Instruct-v0.1",
    "messages": [{"role": "user", "content": "OlÃ¡"}],
    "temperature": 0.7,
    "max_tokens": 128
  }'
```

---

## ğŸ“ˆ Benchmark de Performance

### LatÃªncia de Resposta (ms)

Hardware: Intel Xeon 8c, 32GB RAM, sem GPU

```
Modelo         | Primeira Token | Tokens/sec
--------------|----------------|----------
llama2-7b      | 150ms          | 45 tok/s
mistral-7b     | 180ms          | 40 tok/s
neural-chat-7b | 200ms          | 38 tok/s
dolphin-mixtral| 400ms          | 20 tok/s
openhermes-34b | 800ms          | 12 tok/s
```

Com GPU (RTX 4090):

```
Modelo         | Primeira Token | Tokens/sec
--------------|----------------|----------
llama2-7b      | 30ms           | 150 tok/s
mistral-7b     | 40ms           | 130 tok/s
openhermes-34b | 100ms          | 80 tok/s
llama2-70b     | 120ms          | 60 tok/s
```

### MemÃ³ria Requerida

```
Modelo              | RAM (full) | RAM (Q4) | GPU Recomendada
--------------------|-----------|----------|----------------
llama2-7b           | 28GB       | 3.8GB    | RTX 3060 (6GB)
mistral-7b          | 28GB       | 3.8GB    | RTX 3060 (6GB)
neural-chat-7b      | 28GB       | 4.7GB    | RTX 3060 (6GB)
dolphin-mixtral-8.7b| 35GB       | 8.7GB    | RTX 4070 (12GB)
openhermes-34b      | 136GB      | 19GB     | RTX 4090 (48GB)
llama2-70b          | 280GB      | 39GB     | RTX 6000 Ada (48GB)
```

---

## ğŸ¯ Decision Tree

```
â”Œâ”€ VocÃª tem GPU? â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                          â”‚
â””â”€ NÃ£o                                     â”‚ Sim
   â”‚                                       â”‚
   â”œâ”€ OrÃ§amento?                          â”‚ â”œâ”€ Quanto de VRAM?
   â”‚  â”‚                                    â”‚ â”‚
   â”‚  â”œâ”€ Baixo (<$100/mÃªs)                â”‚ â”‚ â”œâ”€ <12GB
   â”‚  â”‚  â””â”€ Use: mistral-7b               â”‚ â”‚ â”‚  â””â”€ Use: dolphin-mixtral
   â”‚  â”‚                                    â”‚ â”‚ â”‚
   â”‚  â”œâ”€ MÃ©dio ($100-500)                 â”‚ â”‚ â”œâ”€ 12-24GB
   â”‚  â”‚  â””â”€ Use: mistral + GPU local      â”‚ â”‚ â”‚  â””â”€ Use: openhermes-34b
   â”‚  â”‚                                    â”‚ â”‚ â”‚
   â”‚  â””â”€ Alto (>$500)                     â”‚ â”‚ â””â”€ >48GB
   â”‚     â””â”€ Use: openhermes com CPU       â”‚ â”‚    â””â”€ Use: llama2-70b
   â”‚                                       â”‚ â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€ Use modelo com GPU
```

---

## ğŸ”Œ Exemplo de IntegraÃ§Ã£o

### LangChain + Ollama

```python
from langchain.llms import Ollama
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA

# LLM
llm = Ollama(
    base_url="http://localhost:11434",
    model="mistral",
    temperature=0.3,
    top_p=0.9,
    num_ctx=2048
)

# Embeddings
embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-base-pt-v1.5"
)

# Vector Store
vectorstore = Chroma(
    embedding_function=embeddings,
    persist_directory="./db_chroma"
)

# RAG Chain
qa = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vectorstore.as_retriever(k=5),
    return_source_documents=True
)

# Query
result = qa({"query": "Qual Ã© a portaria X?"})
print(result["result"])
```

---

## ğŸ“‹ Checklist de SeleÃ§Ã£o

```
[ ] Defini o setor/domÃ­nio de aplicaÃ§Ã£o
[ ] Identifiquei recursos de hardware disponÃ­veis
[ ] Testei modelos localmente antes de produÃ§Ã£o
[ ] Comparei latÃªncia vs qualidade
[ ] Escolhi embedding model compatÃ­vel
[ ] Preparei dataset de testes
[ ] Documentei configuraÃ§Ãµes escolhidas
[ ] Planejei backup/updates de modelos
[ ] Configurei monitoramento
[ ] Defini SLA de performance
```

---

**Ãšltima AtualizaÃ§Ã£o:** Janeiro 2026  
**Status dos Modelos:** Verificados e testados
